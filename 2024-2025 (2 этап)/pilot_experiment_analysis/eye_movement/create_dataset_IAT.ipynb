{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename='log.txt', level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_block_number(x):\n",
    "    if x == 1:\n",
    "        return 1\n",
    "    elif x in [2, 3, 4]:\n",
    "        return 2\n",
    "    elif x == 5:\n",
    "        return 3\n",
    "    elif x in [6, 7, 8]:\n",
    "        return 4\n",
    "    elif x == 9:\n",
    "        return 5\n",
    "    elif x in [10, 11, 12]:\n",
    "        return 6\n",
    "    elif x == 13:\n",
    "        return 7\n",
    "    elif x in [14, 15, 16]:\n",
    "        return 8\n",
    "    else:\n",
    "        return None  # На случай, если значение не входит в указанные диапазоны\n",
    "\n",
    "def add_iat_exp(x):\n",
    "    if x in [1, 3, 5, 7]:\n",
    "        return 'train'\n",
    "    elif x in [2, 4, 6, 8]:\n",
    "        return 'main'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_trials(df):\n",
    "    logging.info('Рассчёт RT и обработка button...')\n",
    "    # Группируем строки по 'Total Trial Number'\n",
    "    grouped = df.groupby('Total Trial Number')\n",
    "    \n",
    "    # Для каждой группы (блока) считаем сумму RT и проверяем наличие wrong button\n",
    "    for name, group in grouped:\n",
    "        total_rt = group['RT'].sum()  # Считаем сумму RT для блока\n",
    "                \n",
    "        # Заменяем значения RT на сумму RT для всех строк группы\n",
    "        df.loc[df['Total Trial Number'] == name, 'RT'] = total_rt\n",
    "        \n",
    "        # Если в блоке есть хотя бы один wrong button, заменяем все correct button на wrong button\n",
    "        if (group['Trial Result'] == 'wrong button').any(): # Проверяем наличие wrong button\n",
    "            df.loc[(df['Total Trial Number'] == name) & (df['Trial Result'] == 'correct button'), 'Trial Result'] = 'wrong button'\n",
    "    return df\n",
    "\n",
    "def add_iat_acc(x):\n",
    "    if x == 'correct button':\n",
    "        return 1 \n",
    "    elif x == 'wrong button':\n",
    "        return 0\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def add_trial_valence(row):\n",
    "    if row['Block Number Modified'] in [1, 2, 7, 8]:\n",
    "        return 'positive'\n",
    "    elif row['Block Number Modified'] in [3, 4, 5, 6]:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def determine_iat_cb(row):\n",
    "    if row['IAT_results'] > 0 and row['Trial Valence'] == 'positive':\n",
    "        return 'match'\n",
    "    elif row['IAT_results'] < 0 and row['Trial Valence'] == 'negative':\n",
    "        return 'match'\n",
    "    elif row['IAT_results'] < 0 and row['Trial Valence'] == 'positive':\n",
    "        return 'mismatch'\n",
    "    elif row['IAT_results'] > 0 and row['Trial Valence'] == 'negative':\n",
    "        return 'mismatch'\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "def Vac2Dataset(df, xls, fixation_sheets, saccade_sheets):\n",
    "    df = df.copy()\n",
    "    # Создание списка для хранения строк\n",
    "    rows = []\n",
    "\n",
    "    logging.info('Read Fixations...')\n",
    "    fixation_sheets_data = [pd.read_excel(xls, sheet_name=sheet) for sheet in tqdm(fixation_sheets, total=len(fixation_sheets))]\n",
    "    logging.info('Read Saccades...')\n",
    "    saccade_sheets_data = [pd.read_excel(xls, sheet_name=sheet) for sheet in tqdm(saccade_sheets, total=len(saccade_sheets))]\n",
    "    logging.info('Processing fixation and saccade lists...')\n",
    "    # Для каждого индекса Base_Report будем копировать строки столько раз, сколько нужно для фиксации и саккад\n",
    "    for idx in tqdm(range(df.shape[0]), total=df.shape[0]):\n",
    "        # Получаем текущие данные Fixation и Saccade\n",
    "        fixation_df = fixation_sheets_data[idx]\n",
    "        saccade_df = saccade_sheets_data[idx]\n",
    "\n",
    "        # Получаем текущую строку Base_Report\n",
    "        base_row = df.iloc[[idx]].copy()  # Копируем строку как DataFrame\n",
    "\n",
    "        # Определяем количество строк в Fixation и Saccade для данного индекса\n",
    "        fixation_rows = fixation_df if not fixation_df.empty else pd.DataFrame(columns=fixation_df.columns)\n",
    "        saccade_rows = saccade_df if not saccade_df.empty else pd.DataFrame(columns=saccade_df.columns)\n",
    "\n",
    "        # Максимум строк для этого индекса\n",
    "        max_rows_local = max(len(fixation_rows), len(saccade_rows))\n",
    "\n",
    "        # Если ни Fixation, ни Saccade не имеют строк, просто добавляем строку Base_Report\n",
    "        if max_rows_local == 0:\n",
    "            rows.append(base_row)\n",
    "\n",
    "        for i in range(max_rows_local):\n",
    "            # Создаем копию строки Base_Report для каждой итерации\n",
    "            new_row = base_row.copy()\n",
    "\n",
    "            # Добавляем данные из Fixation, если они есть\n",
    "            if i < len(fixation_rows):\n",
    "                for col in fixation_df.columns:\n",
    "                    new_row[f\"f_{col}\"] = fixation_rows.iloc[i][col]\n",
    "            else:\n",
    "                for col in fixation_df.columns:\n",
    "                    new_row[f\"f_{col}\"] = None\n",
    "\n",
    "            # Добавляем данные из Saccade, если они есть\n",
    "            if i < len(saccade_rows):\n",
    "                for col in saccade_df.columns:\n",
    "                    new_row[f\"s_{col}\"] = saccade_rows.iloc[i][col]\n",
    "            else:\n",
    "                for col in saccade_df.columns:\n",
    "                    new_row[f\"s_{col}\"] = None\n",
    "\n",
    "            # Добавляем строку в список\n",
    "            rows.append(new_row)\n",
    "    [row for row in rows if row.isnull().all(axis=None) or row.empty]\n",
    "    # Конкатенируем все строки в один DataFrame\n",
    "    final_df = pd.concat(rows, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "expected_columns = [\n",
    "    'Block Number', 'Trial Number', 'Total Trial Number', 'Targetword', 'Category', \n",
    "    'Index', 'Trial Result', 'Pressed Button', 'RT', 'Button Bad', 'Button Good', 'Button Vac'\n",
    "]\n",
    "def find_header_row(excel_file):\n",
    "    data = pd.read_excel(excel_file, header=None)  # Читаем файл без заголовков\n",
    "    for i, row in enumerate(data.values):\n",
    "        # Проверяем, что все ожидаемые столбцы присутствуют в этой строке\n",
    "        if all(col in row for col in expected_columns):\n",
    "            return i\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_participant_data(excel_file, participant, pivot_df):\n",
    "    xls = pd.ExcelFile(excel_file)\n",
    "    header_row = find_header_row(excel_file)\n",
    "\n",
    "    if header_row is None:\n",
    "        logging.error(f\"Ошибка: не удалось найти строку с заголовками в файле участника {participant}\")\n",
    "        return None\n",
    "    \n",
    "    base_report_df = pd.read_excel(xls, sheet_name=0, skiprows=header_row)\n",
    "    logging.info('Удаление незначимых строк...')\n",
    "    base_report_df = base_report_df[~base_report_df['Block Number'].apply(lambda x: isinstance(x, str) and 'Check' in x)]\n",
    "    logging.info(f'Итоговый предочищенный размер таблицы составил:\\n\\tСтолбцов: {base_report_df.shape[0]}, Строк: {base_report_df.shape[1]}')\n",
    "\n",
    "    logging.info('Создание столбца: Block Number Modified...')\n",
    "    base_report_df['Block Number Modified'] = base_report_df['Block Number'].apply(modify_block_number)\n",
    "    logging.info('Создание столбца: IAT_EXP...')\n",
    "    base_report_df['IAT_EXP'] = base_report_df['Block Number Modified'].apply(add_iat_exp)\n",
    "    \n",
    "    base_report_df = process_trials(base_report_df)\n",
    "    base_report_df.rename(columns={'RT': 'IAT_RT'}, inplace=True)\n",
    "    logging.info('Создание столбца: IAT_ACCURACY...')\n",
    "    base_report_df['IAT_ACCURACY'] = base_report_df['Trial Result'].apply(add_iat_acc)\n",
    "\n",
    "    logging.info('Считывание переменных IAT_results, IAT_results2, IAT_streght из pivot таблицы...')\n",
    "    participant_data = pivot_df[pivot_df['ID'] == participant]\n",
    "    if not participant_data.empty:\n",
    "        base_report_df['IAT_results'] = participant_data['IAT_results'].values[0]\n",
    "        base_report_df['IAT_streght'] = participant_data['IAT_streght'].values[0]\n",
    "        base_report_df['IAT_results2'] = participant_data['IAT_results2'].values[0]\n",
    "\n",
    "    logging.info('Создание столбцов Trial Valence, IAT_cb...')\n",
    "    base_report_df['Trial Valence'] = base_report_df.apply(add_trial_valence, axis=1)\n",
    "    base_report_df['IAT_cb'] = base_report_df.apply(determine_iat_cb, axis=1)\n",
    "\n",
    "    fixation_sheets = [sheet for sheet in xls.sheet_names if sheet.startswith('FixationList')]\n",
    "    saccade_sheets = [sheet for sheet in xls.sheet_names if sheet.startswith('SaccadeList')]\n",
    "    logging.info(f'Size of Fixation List: {len(fixation_sheets)}')\n",
    "    logging.info(f'Size of Saccade List: {len(saccade_sheets)}')\n",
    "    logging.info(f'Size of Dataset: {len(base_report_df)}')\n",
    "\n",
    "    \n",
    "    final_data = Vac2Dataset(base_report_df, xls, fixation_sheets, saccade_sheets)\n",
    "    agg_funcs = {\n",
    "        'f_Duration': ['count', 'first', 'mean', 'sum'],\n",
    "        's_Duration': ['count', 'mean', 'sum'],\n",
    "        's_Amplitude': ['mean', 'sum'],\n",
    "        's_Distance': ['mean', 'sum'],\n",
    "        's_Peak Velocity': ['mean', 'sum']\n",
    "    }\n",
    "    # Агрегация по метрикам\n",
    "    metrics = final_data.groupby('Total Trial Number').agg(agg_funcs)\n",
    "    # Переименование столбцов для удобства\n",
    "    metrics.columns = [\n",
    "        'fixation_count', 'first_FD', 'mean_FD', 'total_FD',\n",
    "        'saccades_count', 'mean_SD', 'total_SD',\n",
    "        'mean_SA', 'total_SA',\n",
    "        'mean_SDist', 'total_SDist',\n",
    "        'mean_PV', 'total_PV'\n",
    "    ]\n",
    "    # Сброс индекса, чтобы сохранить trial как столбец\n",
    "    metrics = metrics.reset_index()\n",
    "    metrics = pd.merge(base_report_df, metrics, on='Total Trial Number', how='left')\n",
    "    \n",
    "    logging.info('Сортировка столбцов...')\n",
    "    columns_order = ['Block Number', 'Trial Valence', 'IAT_results', 'IAT_results2', 'IAT_cb', 'IAT_streght', 'IAT_EXP',\n",
    "                 'Trial Number', 'Total Trial Number', 'Block Number Modified', 'Targetword', 'Category', 'Index', \n",
    "                 'Trial Result', 'IAT_ACCURACY', 'Pressed Button', 'IAT_RT', 'Button Bad', 'Button Good', 'Button Vac', \n",
    "                 'f_Index', 'f_Center X', 'f_Center Y', 'f_Start Time', 'f_Duration', 'f_End Time',\n",
    "                 's_Index', 's_Start Time', 's_Start X', 's_Start Y', 's_Duration', 's_Amplitude', 's_Distance', 's_Peak Velocity',\n",
    "                 's_Average Velocity', 's_End Time', 's_End X','s_End Y',]\n",
    "    \n",
    "    if len(columns_order) != final_data.shape[1]:\n",
    "        logging.warning(f'Will be dropped! {[j for j in final_data.columns if not (j in  columns_order)]}')\n",
    "    final_data = final_data[columns_order]\n",
    "\n",
    "    final_data.reset_index(drop=True, inplace=True)\n",
    "    return final_data, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 409/409 [00:00<00:00, 595.40it/s]\n",
      "100%|██████████| 409/409 [00:01<00:00, 405.35it/s]\n",
      "100%|██████████| 409/409 [00:02<00:00, 150.85it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 397/397 [00:00<00:00, 627.71it/s]\n",
      "100%|██████████| 397/397 [00:00<00:00, 429.27it/s]\n",
      "100%|██████████| 397/397 [00:02<00:00, 158.87it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 429/429 [00:00<00:00, 599.14it/s]\n",
      "100%|██████████| 429/429 [00:00<00:00, 449.93it/s]\n",
      "100%|██████████| 429/429 [00:06<00:00, 70.49it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 389/389 [00:00<00:00, 609.04it/s]\n",
      "100%|██████████| 389/389 [00:00<00:00, 475.55it/s]\n",
      "100%|██████████| 389/389 [00:05<00:00, 68.13it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 388/388 [00:00<00:00, 586.99it/s]\n",
      "100%|██████████| 388/388 [00:00<00:00, 388.99it/s]\n",
      "100%|██████████| 388/388 [00:11<00:00, 34.87it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 392/392 [00:00<00:00, 549.65it/s]\n",
      "100%|██████████| 392/392 [00:00<00:00, 456.95it/s]\n",
      "100%|██████████| 392/392 [00:03<00:00, 98.70it/s] \n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 403/403 [00:00<00:00, 573.25it/s]\n",
      "100%|██████████| 403/403 [00:00<00:00, 429.64it/s]\n",
      "100%|██████████| 403/403 [00:03<00:00, 102.21it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 385/385 [00:00<00:00, 592.35it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 450.83it/s]\n",
      "100%|██████████| 385/385 [00:10<00:00, 37.05it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 423/423 [00:00<00:00, 624.81it/s]\n",
      "100%|██████████| 423/423 [00:01<00:00, 404.84it/s]\n",
      "100%|██████████| 423/423 [00:03<00:00, 114.72it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 399/399 [00:00<00:00, 603.31it/s]\n",
      "100%|██████████| 399/399 [00:00<00:00, 474.87it/s]\n",
      "100%|██████████| 399/399 [00:05<00:00, 77.94it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 408/408 [00:00<00:00, 500.70it/s]\n",
      "100%|██████████| 408/408 [00:00<00:00, 424.45it/s]\n",
      "100%|██████████| 408/408 [00:06<00:00, 67.26it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 402/402 [00:00<00:00, 631.09it/s]\n",
      "100%|██████████| 402/402 [00:00<00:00, 453.92it/s]\n",
      "100%|██████████| 402/402 [00:03<00:00, 130.68it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 385/385 [00:00<00:00, 510.76it/s]\n",
      "100%|██████████| 385/385 [00:00<00:00, 457.68it/s]\n",
      "100%|██████████| 385/385 [00:05<00:00, 70.42it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 411/411 [00:00<00:00, 643.18it/s]\n",
      "100%|██████████| 411/411 [00:00<00:00, 511.17it/s]\n",
      "100%|██████████| 411/411 [00:05<00:00, 76.47it/s] \n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 388/388 [00:00<00:00, 618.06it/s]\n",
      "100%|██████████| 388/388 [00:00<00:00, 472.02it/s]\n",
      "100%|██████████| 388/388 [00:04<00:00, 84.30it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 392/392 [00:00<00:00, 668.47it/s]\n",
      "100%|██████████| 392/392 [00:00<00:00, 512.51it/s]\n",
      "100%|██████████| 392/392 [00:05<00:00, 72.54it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 391/391 [00:00<00:00, 513.79it/s]\n",
      "100%|██████████| 391/391 [00:00<00:00, 416.35it/s]\n",
      "100%|██████████| 391/391 [00:05<00:00, 65.46it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 397/397 [00:00<00:00, 650.11it/s]\n",
      "100%|██████████| 397/397 [00:00<00:00, 467.61it/s]\n",
      "100%|██████████| 397/397 [00:05<00:00, 70.38it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 194/194 [00:00<00:00, 740.35it/s]\n",
      "100%|██████████| 194/194 [00:00<00:00, 510.53it/s]\n",
      "100%|██████████| 194/194 [00:03<00:00, 53.90it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 418/418 [00:00<00:00, 666.93it/s]\n",
      "100%|██████████| 418/418 [00:01<00:00, 416.80it/s]\n",
      "100%|██████████| 418/418 [00:04<00:00, 100.63it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 384/384 [00:00<00:00, 594.59it/s]\n",
      "100%|██████████| 384/384 [00:00<00:00, 444.96it/s]\n",
      "100%|██████████| 384/384 [00:03<00:00, 123.77it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 398/398 [00:00<00:00, 667.79it/s]\n",
      "100%|██████████| 398/398 [00:00<00:00, 453.83it/s]\n",
      "100%|██████████| 398/398 [00:03<00:00, 117.07it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 405/405 [00:00<00:00, 545.66it/s]\n",
      "100%|██████████| 405/405 [00:00<00:00, 436.89it/s]\n",
      "100%|██████████| 405/405 [00:04<00:00, 87.08it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 390/390 [00:00<00:00, 597.27it/s]\n",
      "100%|██████████| 390/390 [00:00<00:00, 468.19it/s]\n",
      "100%|██████████| 390/390 [00:05<00:00, 75.94it/s] \n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 416/416 [00:00<00:00, 586.14it/s]\n",
      "100%|██████████| 416/416 [00:00<00:00, 422.80it/s]\n",
      "100%|██████████| 416/416 [00:03<00:00, 107.99it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 396/396 [00:00<00:00, 539.48it/s]\n",
      "100%|██████████| 396/396 [00:00<00:00, 437.09it/s]\n",
      "100%|██████████| 396/396 [00:07<00:00, 50.73it/s]\n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n",
      "100%|██████████| 423/423 [00:00<00:00, 554.60it/s]\n",
      "100%|██████████| 423/423 [00:00<00:00, 444.33it/s]\n",
      "100%|██████████| 423/423 [00:07<00:00, 59.98it/s] \n",
      "C:\\Users\\searg\\AppData\\Local\\Temp\\ipykernel_24236\\297160089.py:128: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  final_df = pd.concat(rows, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# Путь к основной папке с участниками\n",
    "participants_dir = 'EEG_CB_IAT'  # Замените на ваш путь\n",
    "pivot_file  = 'CB_EEG_RSF.xlsx'\n",
    "\n",
    "temp_dir = 'temp'\n",
    "# Проверяем, существует ли папка, и если нет, создаем её\n",
    "if not os.path.exists(temp_dir):\n",
    "    os.makedirs(temp_dir)\n",
    "    logging.info(f\"Папка {temp_dir} создана.\")\n",
    "else:\n",
    "    logging.info(f\"Папка {temp_dir} уже существует.\")\n",
    "dataset_metrcis_list = []\n",
    "dataset_list = []\n",
    "names_participant = []\n",
    "# Проход по каждой папке с участником\n",
    "for participant in os.listdir(participants_dir):\n",
    "    participant_path = os.path.join(participants_dir, participant)\n",
    "\n",
    "    # Проверяем, что это папка\n",
    "    if os.path.isdir(participant_path):\n",
    "        vac_2_path = os.path.join(participant_path, 'Vac_2')\n",
    "        pivot_df = pd.read_excel(pivot_file)\n",
    "        # Проверяем наличие папки Vac_2 внутри папки участника\n",
    "        if not os.path.exists(vac_2_path) or not os.path.isdir(vac_2_path):\n",
    "            logging.error(f\"Ошибка: У участника {participant} отсутствует папка Vac_2\")\n",
    "        else:\n",
    "            # Получаем список файлов в папке Vac_2\n",
    "            files = os.listdir(vac_2_path)\n",
    "            \n",
    "            # Проверяем количество файлов в папке Vac_2\n",
    "            if len(files) != 1:\n",
    "                logging.error(f\"Ошибка: У участника {participant} в папке Vac_2 неправильное количество файлов (найдено {len(files)})\")\n",
    "\n",
    "            elif pivot_df[pivot_df['ID'] == participant].empty:\n",
    "                logging.error(f\"Ошибка: Участник с ID {participant} не найден в {pivot_file}\")\n",
    "\n",
    "            #elif os.path.exists(f'temp/{participant}.xlsx'):\n",
    "            #    logging.info(f'Уже существует {participant}')\n",
    "\n",
    "            else:\n",
    "                participant_data, metrics_data = preprocessing_participant_data(os.path.join(vac_2_path, files[0]), participant, pivot_df)\n",
    "                if not (participant_data is None):\n",
    "                    logging.info(f'Данные участника {participant} обработаны!')\n",
    "                    dataset_list.append(participant_data)\n",
    "                    dataset_metrcis_list.append(metrics_data)\n",
    "                    names_participant.append(participant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вывод списков\n",
    "logging.info(f'Список участников: {names_participant}')\n",
    "\n",
    "# Добавляем имя участника как новый столбец в каждый DataFrame и объединяем таблицы\n",
    "for i, df in enumerate(dataset_list):\n",
    "    df['Participant'] = os.path.splitext(names_participant[i])[0]\n",
    "    # Перемещаем столбец 'Participant' на первое место\n",
    "    cols = df.columns.tolist()\n",
    "    cols = ['Participant'] + [col for col in cols if col != 'Participant']\n",
    "    dataset_list[i] = df[cols]\n",
    "\n",
    "    dataset_metrcis_list[i]['Participant'] = os.path.splitext(names_participant[i])[0]\n",
    "    # Перемещаем столбец 'Participant' на первое место\n",
    "    cols = dataset_metrcis_list[i].columns.tolist()\n",
    "    cols = ['Participant'] + [col for col in cols if col != 'Participant']\n",
    "    dataset_metrcis_list[i] = dataset_metrcis_list[i][cols]\n",
    "\n",
    "# Объединяем все таблицы в одну\n",
    "combined_df = pd.concat(dataset_list, ignore_index=True)\n",
    "combined_metrics_df = pd.concat(dataset_metrcis_list, ignore_index=True)\n",
    "combined_df.to_excel('IAT_RAW_EYE.xlsx', index=False)\n",
    "combined_metrics_df.to_excel('IAT_EYE.xlsx', index=False)\n",
    "logging.info('Датасет успешно создан!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем файл и фильтруем строки уровня \"ERROR\"\n",
    "error_lines = []\n",
    "\n",
    "with open('log.txt', 'r') as log_file:\n",
    "    for line in log_file:\n",
    "        if 'ERROR' in line:  # Фильтруем по уровню ошибки\n",
    "            error_lines.append(line)\n",
    "\n",
    "# Выводим все строки с ошибками\n",
    "for error in error_lines:\n",
    "    print(error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
